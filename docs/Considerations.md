## necessary aspects: 

- **structure everything**
	- model data to a specific folder
	- pre-made weights being given to model 
- **unite webinterface** --> only one interface which we can use for all of our 
- split webinterface from Models!! 
- Faster image loading 
- youtube-dl 
- crawler from websites 
- datasets
	- we could take pre-made sample packages 
	- we could 
- regarding datasets 
	- querying from them and returning something from them 
	- we should 
- load image from dataset with given name 
	- process it and then return the result 
- back into the dataset we've created before 
- **provide PEP-Style Guidelines for our project**
- **implementing performance comparisons**
	- time it took to run 
		- detla time supplied 
	- amount of detected objects within a given dataset 
		- details about used weight/ pre trained model 
		- details about runtime 
	- implement site to run a selected image on different models and then showcasing the results with graphs


## Website considerations: 

What is supplied by the website? 
- selection of model to run
- selection of data set to use! 
- data to look at 
- possible data stream / image / video / livefeed 
- requires confidence score
- **object of image** where result will be written to

What is required by the models? 
- data stream 
- confidence score 
- selected data set 
- object classes to detec
- 

What is returned by the models? 
- quantity of detected objects ( matching threshold) 
- modified image  

What is required by website ? 
- modified image 
- quantity of detected objects 